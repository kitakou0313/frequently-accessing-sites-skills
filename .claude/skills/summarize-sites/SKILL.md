---
name: summarize-sites
description: Webサイトのリストから最新の記事を要約し、Markdownレポートとしてエクスポートする。サイト巡回、記事要約、ニュースまとめを求められた場合に使用する。
compatibility: Requires internet access for WebFetch.
---

# summarize-sites

Webサイトのリストから最新の5記事を要約し、Markdownファイルにエクスポートするスキル。

## 実行手順

### 1. URLリストの取得

`$ARGUMENTS` が空でない場合、スペース区切りでURLリストとして使用する。

`$ARGUMENTS` が空の場合、プロジェクトルートの `sites.txt` を `Read` ツールで読み込み、1行1URLとしてURLリストを取得する。空行や `#` で始まるコメント行はスキップする。

URLリストが空の場合はエラーメッセージを表示して終了する。

### 2. 各サイトの記事収集

各URLに対して `WebFetch` を実行し、記事の一覧を取得する。

その後、最新の5記事の詳細を`WebFetch`による取得で確認し、まとめる。
promptには以下を指定する:

```
与えたURLについて、`WebFetch`を利用して記事の内容を取得してください。

各記事について以下の情報をJSON配列で返してください:
- title: 記事タイトル
- date: 公開日（YYYY-MM-DD形式、不明なら "不明"）
- summary: 記事全体の内容の網羅的な要約（2〜3文）
- url: 記事のURL（取得できない場合はサイトURLを使用）

該当する記事がない場合は空の配列 [] を返してください。サイト名も冒頭に「site_name: サイト名」の形式で記載してください。
```

WebFetchがエラーになった場合は、そのサイトについて「取得に失敗しました」と記録し、次のサイトに進む。

### 3. レポートの生成

収集した結果を以下のMarkdown形式にまとめる:

```markdown
# サイト巡回要約レポート — {今日の日付 YYYY-MM-DD}

---

## {サイト名} ({URL})

### {記事タイトル1}
- **日付**: {公開日}
- **要約**: {要約文}
- **リンク**: {記事URL}

### {記事タイトル2}
- **日付**: {公開日}
- **要約**: {要約文}
- **リンク**: {記事URL}

---

## {サイト名2} ({URL2})

> 該当期間の記事は見つかりませんでした。

---
```

記事が見つからなかったサイトには「該当期間の記事は見つかりませんでした。」と記載する。
取得に失敗したサイトには「サイトの取得に失敗しました。」と記載する。

### 4. ファイルの保存

`output/`ディレクトリが存在するか確認する。
存在しない場合。`Bash` で `output/` ディレクトリを作成する（`mkdir -p output`）。

`Write` ツールで `output/summary-{今日の日付 YYYY-MM-DD}.md` にレポートを保存する。

### 5. 完了報告

保存したファイルのパスをユーザーに報告する。対象サイト数、取得成功数、記事総数も合わせて報告する。
