# summarize-sites

Webサイトのリストから直近3日分の記事を要約し、Markdownファイルにエクスポートするスキル。

## 実行手順

### 1. URLリストの取得

`$ARGUMENTS` が空でない場合、スペース区切りでURLリストとして使用する。

`$ARGUMENTS` が空の場合、プロジェクトルートの `sites.txt` を `Read` ツールで読み込み、1行1URLとしてURLリストを取得する。空行や `#` で始まるコメント行はスキップする。

URLリストが空の場合はエラーメッセージを表示して終了する。

### 2. 各サイトの記事収集
各サイトについて、最新の5記事について抽出する。

各URLに対して `WebFetch` を実行する。promptには以下を指定する:

```
このサイトから最新の5記事について記事・ニュースを抽出してください。
記事の内容については`WebFetch`を利用して取得してください。

各記事について以下の情報をJSON配列で返してください:
- title: 記事タイトル
- date: 公開日（YYYY-MM-DD形式、不明なら "不明"）
- summary: 記事の要約（2〜3文）
- url: 記事のURL（取得できない場合はサイトURLを使用）

該当する記事がない場合は空の配列 [] を返してください。サイト名も冒頭に「site_name: サイト名」の形式で記載してください。
```

WebFetchがエラーになった場合は、そのサイトについて「取得に失敗しました」と記録し、次のサイトに進む。

### 3. レポートの生成

収集した結果を以下のMarkdown形式にまとめる:

```markdown
# サイト巡回要約レポート — {今日の日付 YYYY-MM-DD}

対象期間: {開始日} 〜 {今日の日付}

---

## {サイト名} ({URL})

### {記事タイトル1}
- **日付**: {公開日}
- **要約**: {要約文}
- **リンク**: {記事URL}

### {記事タイトル2}
- **日付**: {公開日}
- **要約**: {要約文}
- **リンク**: {記事URL}

---

## {サイト名2} ({URL2})

> 該当期間の記事は見つかりませんでした。

---
```

記事が見つからなかったサイトには「該当期間の記事は見つかりませんでした。」と記載する。
取得に失敗したサイトには「サイトの取得に失敗しました。」と記載する。

### 4. ファイルの保存

`Bash` で `output/` ディレクトリを作成する（`mkdir -p output`）。

`Write` ツールで `output/summary-{今日の日付 YYYY-MM-DD}.md` にレポートを保存する。

### 5. 完了報告

保存したファイルのパスをユーザーに報告する。対象サイト数、取得成功数、記事総数も合わせて報告する。
